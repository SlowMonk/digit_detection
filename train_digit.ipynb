{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import h5py\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from utils import read_mat_file, visualize_sample_dataset\n",
    "from dataloader import SVHDDataset\n",
    "import scipy.io\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from model import VGG\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "from torchvision import models, transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "# Train the model\n",
    "from model import VGG16\n",
    "#import cv2 \n",
    "\n",
    "os.environ['TORCH_NNPACK'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/data/omscs_datasets/train/'\n",
    "train_mat_file_path = train_path + 'digitStruct.mat'\n",
    "test_path = '/data/omscs_datasets/train/'\n",
    "test_mat_file_path = test_path + 'digitStruct.mat'\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class SVHDDigitNonDigitDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, mat_file_path, image_dir, mode):\n",
    "        # 파일 열기 및 데이터셋 구조 읽기\n",
    "        with h5py.File(mat_file_path, 'r') as f:\n",
    "            if mode == 'train':\n",
    "                print('## train ##')\n",
    "                self.digitStruct = f['digitStruct']\n",
    "                self.bbox_refs = [obj_ref[0] for _, obj_ref in enumerate(self.digitStruct['bbox'])]\n",
    "                self.name_refs = [obj_ref[0] for _, obj_ref in enumerate(self.digitStruct['name'])]\n",
    "            else:\n",
    "                print('## test ##')\n",
    "                self.digitStruct = f\n",
    "                self.bbox_refs = [f[key][()] for key in f['bbox'].keys()]\n",
    "                self.name_refs = [f[key][()] for key in f['name'].keys()]\n",
    "\n",
    "            self.length = len(self.digitStruct)\n",
    "\n",
    "        self.file = h5py.File(mat_file_path, 'r')  # 파일 따로 열기\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 이미지 및 레이블 정보 액세스\n",
    "        bbox_data = self.file[self.bbox_refs[idx]]\n",
    "\n",
    "        num_boxes = bbox_data['height'].shape[0]\n",
    "\n",
    "        digit_samples = []\n",
    "        non_digit_samples = []\n",
    "\n",
    "        for i in (range(num_boxes)):\n",
    "            # bbox 정보 로드\n",
    "            height_ref = bbox_data['height'][i]\n",
    "            width_ref = bbox_data['width'][i]\n",
    "            top_ref = bbox_data['top'][i]\n",
    "            left_ref = bbox_data['left'][i]\n",
    "            label_ref = bbox_data['label'][i]\n",
    "\n",
    "            try:\n",
    "                height = self.file[height_ref[0]][()].item()\n",
    "                width = self.file[width_ref[0]][()].item()\n",
    "                top = self.file[top_ref[0]][()].item()\n",
    "                left = self.file[left_ref[0]][()].item()\n",
    "                label = self.file[label_ref[0]][()].item()\n",
    "            except:\n",
    "                height = height_ref[0]\n",
    "                width = width_ref[0]\n",
    "                top = top_ref[0]\n",
    "                left = left_ref[0]\n",
    "                label = label_ref[0]\n",
    "\n",
    "            # 이미지 로드 및 크롭\n",
    "            image_path = os.path.join(self.image_dir, str(idx + 1) + '.png')\n",
    "            image = Image.open(image_path)\n",
    "            cropped_image = image.crop((left, top, left + width, top + height))\n",
    "            cropped_image = self.transform(cropped_image)\n",
    "\n",
    "            digit_sample = {\n",
    "                'image': cropped_image,\n",
    "                'label': 1,\n",
    "                'name': image_path\n",
    "            }\n",
    "            digit_samples.append(digit_sample)\n",
    "    \n",
    "        if num_boxes == 1:\n",
    "            try:\n",
    "                # 임의의 위치와 크기로 박스 생성\n",
    "                left = random.randint(0, image.size[0] - 32)\n",
    "                top = random.randint(0, image.size[1] - 32)\n",
    "\n",
    "                # Digit와 겹치는지 확인\n",
    "                try:\n",
    "                    d_height = self.file[height_ref[0]][()].item()\n",
    "                    d_width = self.file[width_ref[0]][()].item()\n",
    "                    d_top = self.file[top_ref[0]][()].item()\n",
    "                    d_left = self.file[left_ref[0]][()].item()\n",
    "                    label = self.file[label_ref[0]][()].item()\n",
    "                except:\n",
    "                    d_height = height_ref[0]\n",
    "                    d_width = width_ref[0]\n",
    "                    d_top = top_ref[0]\n",
    "                    d_left = left_ref[0]\n",
    "                    label = label_ref[0]\n",
    "\n",
    "                if not (left + 32 <= d_left or left >= d_left + d_width or\n",
    "                        top + 32 <= d_top or top >= d_top + d_height):\n",
    "                    pass\n",
    "\n",
    "                # Non-digit 영역 크롭\n",
    "                cropped_image = image.crop((left, top, left + 32, top + 32))\n",
    "                cropped_image = self.transform(cropped_image)\n",
    "\n",
    "                non_digit_sample = {\n",
    "                    'image': cropped_image,\n",
    "                    'label': 0,\n",
    "                    'name': image_path\n",
    "                }\n",
    "                non_digit_samples.append(non_digit_sample)\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # Digit 및 Non-Digit 샘플 결합\n",
    "        return digit_samples +  non_digit_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## train ##\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SVHDDigitNonDigitDataset(train_mat_file_path, train_path,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33402it [02:56, 189.62it/s]          \n"
     ]
    }
   ],
   "source": [
    "train_images = []\n",
    "train_labels = []\n",
    "for t in tqdm(train_dataset):\n",
    "    for tt in t:\n",
    "        train_images.append(tt['image'])\n",
    "        train_labels.append(tt['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3828 73257\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.count(0), train_labels.count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32]) 1\n"
     ]
    }
   ],
   "source": [
    "num = 200\n",
    "print(train_images[num].shape, train_labels[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f39128ab6a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np_image = train_images[num].numpy().transpose(1,2,0)\n",
    "plt.imshow(np_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80913 80913\n",
      "7656 73257\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# 데이터 증강을 위한 변환 정의\n",
    "augmentation_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n",
    "])\n",
    "\n",
    "# 레이블 별로 이미지 분류\n",
    "images_label_0 = [train_images[i] for i in range(len(train_labels)) if train_labels[i] == 0]\n",
    "images_label_1 = [train_images[i] for i in range(len(train_labels)) if train_labels[i] == 1]\n",
    "\n",
    "# 레이블 0의 이미지에 대한 데이터 증강 수행\n",
    "augmented_images_label_0 = []\n",
    "for img in images_label_0:\n",
    "    augmented_image = augmentation_transforms(img)\n",
    "    augmented_images_label_0.append(augmented_image)\n",
    "\n",
    "# 증강된 이미지와 원본 이미지를 합침\n",
    "balanced_images_label_0 = images_label_0 + augmented_images_label_0\n",
    "\n",
    "# 이미지와 레이블 합치기\n",
    "balanced_train_images = balanced_images_label_0 + images_label_1\n",
    "balanced_train_labels = [0] * len(balanced_images_label_0) + [1] * len(images_label_1)\n",
    "\n",
    "# 결과 확인\n",
    "print(len(balanced_train_images), len(balanced_train_labels))\n",
    "print(balanced_train_labels.count(0), balanced_train_labels.count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        #if self.transform:\n",
    "        #    print('image:', image.shape)\n",
    "        #    image = self.transform(image)\n",
    "\n",
    "        # Return the image and label as a dictionary or tuple\n",
    "        sample = {'image': image, 'label': label}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80913 80913\n"
     ]
    }
   ],
   "source": [
    "print(len(balanced_train_images), len(balanced_train_labels))\n",
    "train_dataset = CustomDataset(balanced_train_images, balanced_train_labels, transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data:64730, test_data:16183\n"
     ]
    }
   ],
   "source": [
    "# Specify the lengths of the training and testing sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "test_size = len(train_dataset) - train_size\n",
    "\n",
    "# Use random_split to split the dataset\n",
    "train_data, test_data = random_split(train_dataset, [train_size, test_size])\n",
    "print(f'train_data:{len(train_data)}, test_data:{len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506 127\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create a DataLoader for your dataset\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=128, shuffle=False)\n",
    "print(len(train_loader), len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            inputs, labels = batch['image'], batch['label']\n",
    "            inputs = inputs.to(torch.float32).to(device)\n",
    "            labels = labels.to(torch.float32).to(device)\n",
    "            labels = labels.long()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, criterion, optimizer, device, num_epochs=5):\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        print('## epoch ##', epoch)\n",
    "        \n",
    "        for batch in tqdm(train_loader):\n",
    "            inputs, labels = batch['image'], batch['label']\n",
    "            inputs = inputs.to(torch.float32).to(device)\n",
    "            labels = labels.to(torch.float32).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            labels = labels.long()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = test_accuracy(model, test_loader, device)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss}, Test Accuracy: {epoch_accuracy}%\")\n",
    "        \n",
    "        if epoch_accuracy > best_accuracy:\n",
    "            best_accuracy = epoch_accuracy\n",
    "            torch.save(model.state_dict(), 'best_digit_recog_model.pth')\n",
    "            print(f\"New best model saved with accuracy: {epoch_accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG16(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace=True)\n",
       "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (36): ReLU(inplace=True)\n",
       "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (39): ReLU(inplace=True)\n",
       "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace=True)\n",
       "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = VGG(num_classes=11).to(device)\n",
    "model =  VGG16(num_classes=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# Train the model\n",
    "model.to(device)\n",
    "# train(model, train_loader, criterion, optimizer, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## epoch ## 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/506 [00:00<?, ?it/s]/root/miniconda3/envs/cv_proj/lib/python3.9/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "100%|██████████| 506/506 [00:09<00:00, 51.81it/s]\n",
      "100%|██████████| 127/127 [00:00<00:00, 144.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.143869395047544, Test Accuracy: 95.85985293208923%\n",
      "New best model saved with accuracy: 95.85985293208923%\n",
      "## epoch ## 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 506/506 [00:09<00:00, 54.04it/s]\n",
      "100%|██████████| 127/127 [00:00<00:00, 151.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.05969843071822828, Test Accuracy: 98.16474077735896%\n",
      "New best model saved with accuracy: 98.16474077735896%\n",
      "## epoch ## 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 506/506 [00:09<00:00, 53.68it/s]\n",
      "100%|██████████| 127/127 [00:00<00:00, 153.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.0424681393541911, Test Accuracy: 98.442810356547%\n",
      "New best model saved with accuracy: 98.442810356547%\n",
      "## epoch ## 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 506/506 [00:09<00:00, 54.01it/s]\n",
      "100%|██████████| 127/127 [00:00<00:00, 153.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.02814301984658899, Test Accuracy: 98.38719644070939%\n",
      "## epoch ## 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 506/506 [00:09<00:00, 54.16it/s]\n",
      "100%|██████████| 127/127 [00:00<00:00, 155.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.02196910385094885, Test Accuracy: 98.38719644070939%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, test_loader, criterion, optimizer, device, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
