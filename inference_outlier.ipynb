{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import h5py\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from utils import read_mat_file, visualize_sample_dataset, vae_loss\n",
    "from dataloader import SVHDDataset\n",
    "import scipy.io\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from model import VGG\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "from torchvision import models, transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "# Train the model\n",
    "from model import VGG16, VAE\n",
    "import cv2 \n",
    "\n",
    "os.environ['TORCH_NNPACK'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/data/omscs_datasets/train/'\n",
    "train_mat_file_path = train_path + 'digitStruct.mat'\n",
    "test_path = '/data/omscs_datasets/train/'\n",
    "test_mat_file_path = test_path + 'digitStruct.mat'\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_anchor_sizes(image, num_sizes=3, size_ratios=[1, 0.75, 0.5]):\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    # 이미지 크기에 기반한 기본 앵커 크기 계산\n",
    "    base_size = min(height, width) // 4  # 예시: 가로, 세로 중 작은 쪽의 1/4\n",
    "\n",
    "    # 여러 크기와 비율의 앵커 박스 계산\n",
    "    anchor_sizes = []\n",
    "    for i in range(1, num_sizes + 1):\n",
    "        size = base_size * i\n",
    "        for ratio in size_ratios:\n",
    "            anchor_width = int(size * ratio)\n",
    "            anchor_height = int(size * (1 / ratio))\n",
    "            anchor_sizes.append((anchor_width, anchor_height))\n",
    "\n",
    "    return anchor_sizes\n",
    "\n",
    "\n",
    "def detect_digit_from_anchor_image_reconstruct(img_path, model,vgg16, output_image_path, device, num):\n",
    "    # Load your model\n",
    "    #model.load_state_dict(torch.load('best_model.pth'))\n",
    "    model.eval()\n",
    "\n",
    "    # Load the image (now in color)\n",
    "    image = cv2.imread(img_path)\n",
    "\n",
    "    # Define the window size and step size\n",
    "    win_size = 16\n",
    "    window_size = (win_size, win_size)\n",
    "    print('image:', image.shape)\n",
    "    window_size = (int(image.shape[0]//5), int(image.shape[1]//5))\n",
    "    step_size = 10\n",
    "\n",
    "    # List to store bounding box coordinates and predictions\n",
    "    bbox_coords = []\n",
    "    final_bbox_coords = []\n",
    "    losses_array = []\n",
    "    predicted_arr = []\n",
    "    red_arr = []\n",
    "\n",
    "    anchor_sizes = calculate_anchor_sizes(image)\n",
    "    anchor_ratios = [1, 0.75, 0.5]\n",
    "\n",
    "    # Sliding window\n",
    "    for y in range(0, image.shape[0] - window_size[1], step_size):\n",
    "        for x in range(0, image.shape[1] - window_size[0], step_size):\n",
    "            for size in anchor_sizes:\n",
    "                for ratio in anchor_ratios:\n",
    "                    # calculate anchor box dimensions\n",
    "                    anchor_width = int(size[0] * ratio)\n",
    "                    anchor_height = int(size[1] * (1 / ratio))\n",
    "\n",
    "                     # Calculate coordinates of the window\n",
    "                    x1 = max(x - anchor_width // 2, 0)\n",
    "                    y1 = max(y - anchor_height // 2, 0)\n",
    "                    x2 = min(x1 + anchor_width, image.shape[1])\n",
    "                    y2 = min(y1 + anchor_height, image.shape[0])\n",
    "\n",
    "                    # Extract and preprocess the window\n",
    "                    #window = image[y:y + window_size[1], x:x + window_size[0]]\n",
    "                    window = image[y1:y2, x1:x2]\n",
    "                    processed_window = cv2.resize(window, (32, 32))\n",
    "                    processed_window = transforms.ToTensor()(processed_window)\n",
    "                    #processed_window = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(processed_window)\n",
    "                    processed_window = processed_window.unsqueeze(0).to(torch.float32).to(device)\n",
    "                    #print('processed_window:', processed_window.shape)\n",
    "                    # Classify the window\n",
    "                    with torch.no_grad():\n",
    "                        x_reconstructed, mu, log_var  = model(processed_window)\n",
    "                        loss = vae_loss(x_reconstructed, processed_window, mu, log_var)\n",
    "                        losses_array.append(loss.item())\n",
    "                        #_, predicted = torch.max(outputs, 1)\n",
    "                        bbox_coords.append((x, y, x + window_size[0], y + window_size[1], loss.item()))\n",
    "\n",
    "                        with torch.no_grad():\n",
    "                            outputs = vgg16(processed_window)\n",
    "                            _, predicted = torch.max(outputs, 1)\n",
    "                            predicted_arr.append(predicted.item())\n",
    "                        \n",
    "\n",
    "    sorted_lst_asc = sorted(losses_array)[::-1]\n",
    "    limit_loss = sorted_lst_asc[num]\n",
    "    for bbox, lss, pred in zip(bbox_coords, losses_array, predicted_arr):\n",
    "        if lss > limit_loss:\n",
    "            final_bbox_coords.append(bbox)\n",
    "            red_arr.append(pred)\n",
    "    # Draw bounding boxes and text on the original image\n",
    "    for (x1, y1, x2, y2, prediction), pred_digit in zip(final_bbox_coords, red_arr):\n",
    "        print('prediction:',pred_digit)\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 1)\n",
    "        cv2.putText(image, str(pred_digit), (x1, y1+10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)\n",
    "\n",
    "    # Save the final image with bounding boxes and text\n",
    "    cv2.imwrite(output_image_path, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "image_channels = 3\n",
    "h_dim = 1024\n",
    "z_dim = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "vae = VAE(image_channels=image_channels, h_dim=h_dim, z_dim=z_dim)\n",
    "vae = vae.to(device)\n",
    "vae.load_state_dict(torch.load('weights/vae_best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16 =  VGG16(num_classes=11).to(device)\n",
    "vgg16.load_state_dict(torch.load('weights/best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (38, 99, 3)\n",
      "prediction: 1\n",
      "prediction: 1\n",
      "prediction: 1\n",
      "prediction: 1\n",
      "prediction: 1\n",
      "prediction: 1\n",
      "prediction: 1\n",
      "prediction: 1\n"
     ]
    }
   ],
   "source": [
    "detect_digit_from_image_reconstruct('test_images/test4.png', vae, vgg16, 'output_with_boxes_reconstruct.png', device, num=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
