{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import h5py\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from utils import read_mat_file, visualize_sample_dataset, vae_loss\n",
    "from dataloader import SVHDDataset\n",
    "import scipy.io\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from model import VGG\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "from torchvision import models, transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "# Train the model\n",
    "from model import VGG16, VAE\n",
    "import cv2 \n",
    "\n",
    "os.environ['TORCH_NNPACK'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/data/omscs_datasets/train/'\n",
    "train_mat_file_path = train_path + 'digitStruct.mat'\n",
    "test_path = '/data/omscs_datasets/train/'\n",
    "test_mat_file_path = test_path + 'digitStruct.mat'\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_digit_from_image_reconstruct(img_path, model, output_image_path, device):\n",
    "    # Load your model\n",
    "    #model.load_state_dict(torch.load('best_model.pth'))\n",
    "    model.eval()\n",
    "\n",
    "    # Load the image (now in color)\n",
    "    image = cv2.imread(img_path)\n",
    "\n",
    "    # Define the window size and step size\n",
    "    win_size = 16\n",
    "    window_size = (win_size, win_size)\n",
    "    step_size = 10\n",
    "\n",
    "    # List to store bounding box coordinates and predictions\n",
    "    bbox_coords = []\n",
    "\n",
    "    # Sliding window\n",
    "    for y in range(0, image.shape[0] - window_size[1], step_size):\n",
    "        for x in range(0, image.shape[1] - window_size[0], step_size):\n",
    "            # Extract and preprocess the window\n",
    "            window = image[y:y + window_size[1], x:x + window_size[0]]\n",
    "            processed_window = cv2.resize(window, (32, 32))\n",
    "            processed_window = transforms.ToTensor()(processed_window)\n",
    "            processed_window = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(processed_window)\n",
    "            processed_window = processed_window.unsqueeze(0).to(torch.float32).to(device)\n",
    "            #print('processed_window:', processed_window.shape)\n",
    "            # Classify the window\n",
    "            with torch.no_grad():\n",
    "                x_reconstructed, mu, log_var  = model(processed_window)\n",
    "                loss = vae_loss(x_reconstructed, processed_window, mu, log_var)\n",
    "                print('outputs:', loss)\n",
    "                print(stop)\n",
    "                #_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # If it's a digit, save the bounding box coordinates and prediction\n",
    "            if predicted.item() == 8:\n",
    "                bbox_coords.append((x, y, x + window_size[0], y + window_size[1], predicted.item()))\n",
    "\n",
    "    # Draw bounding boxes and text on the original image\n",
    "    for (x1, y1, x2, y2, prediction) in bbox_coords:\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 1)\n",
    "        cv2.putText(image, str(prediction), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "    # Save the final image with bounding boxes and text\n",
    "    cv2.imwrite(output_image_path, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "image_channels = 3\n",
    "h_dim = 1024\n",
    "z_dim = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "vae = VAE(image_channels=image_channels, h_dim=h_dim, z_dim=z_dim)\n",
    "vae = vae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_digit_from_image_reconstruct('test1.png', vae, 'output_with_boxes_reconstruct.png', device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
